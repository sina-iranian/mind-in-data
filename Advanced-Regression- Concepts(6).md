# ğŸ“˜ Advanced Regression Concepts â€” Practical Data Science Notes
*(Based on uploaded lectures: Overfitting, 2D/3D Regression, Dummy Variables, F-Regression, and Final Scenario)*

---

## âš–ï¸ 1ï¸âƒ£ Overfitting, Underfitting & Regularization

### ğŸ”¹ Model Fit Spectrum
| Term | Meaning | Problem |
|------|----------|----------|
| **Underfitting** | Model too simple; misses real patterns | Poor training & test performance |
| **Overfitting** | Model too complex; memorizes noise | High training accuracy, poor generalization |

âœ… **Fixes:**  
- Use **train/test split** (e.g., 80/20).  
- Apply **Regularization** (L1 / L2).  
- Simplify model or collect more data.

### ğŸ”¹ Regularization Types
| Type | Formula | Effect |
|------|----------|--------|
| **L1 (Lasso)** | `Loss + Î»Î£|w|` | Shrinks weights, sets some to 0 â†’ feature selection |
| **L2 (Ridge)** | `Loss + Î»Î£wÂ²` | Penalizes large weights â†’ more stable model |

### ğŸ”¹ Core Regression Loss Functions
| Task | Function | Formula |
|-------|-----------|----------|
| Regression | **MSE** | (1/n)Î£(yâˆ’Å·)Â² â€” penalizes large errors |
| Regression | **MAE** | (1/n)Î£|yâˆ’Å·| â€” robust to outliers |
| Regression | **Huber** | Combines MSE + MAE; smooth near 0 |
| Classification | **Binary Cross-Entropy** | âˆ’(1/n)Î£[y log Å· + (1âˆ’y) log (1âˆ’Å·)] |
| Classification | **Hinge (SVM)** | Î£ max(0, 1âˆ’y Å·) |

---

## ğŸ§  2ï¸âƒ£ Trainâ€“Test Split (scikit-learn)
```python
from sklearn.model_selection import train_test_split
import numpy as np

a = np.arange(1, 101)
train, test = train_test_split(a, test_size=0.2, shuffle=False, random_state=42)
